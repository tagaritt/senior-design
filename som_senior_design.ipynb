## Library Installations
## In this section, various libraries are imported and installed. 
## The MiniSOM library is necessairly installed and imported, important as it provides the baseline SOM implementation for the program. 
## Other common machine learning libraries such as Scikit-learn, NumPy, and Pandas have also been imported.

# Installs
!pip install minisom
!pip install matplotlib==3.0.3

# Imports
from minisom import MiniSom
import numpy as np
import sklearn
from sklearn import preprocessing
from sklearn.datasets import fetch_openml
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import classification_report as cr
from sklearn.metrics import accuracy_score as accuracy
import skimage
from skimage.measure import block_reduce
import pandas as pd
import plotly.figure_factory as ff
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import time
import itertools

# Function Instantiation

Here, all the functions that are used in the program are declared for later use. The function 'm_xy' is declared to create a varible that is used in

def dict_for_entropy(som, X, y):
  dict_winner_neuron = {}
  for i in range(10):
    x_i = y==str(i)
    for x, t in zip(X[x_i], y[x_i]):  # scatterplot
      w = som.winner(x)
      if w not in dict_winner_neuron:
        dict_winner_neuron[w] = {}
        if i not in dict_winner_neuron[w]:
          dict_winner_neuron[w][i] = 1
      elif i not in dict_winner_neuron[w]:
        dict_winner_neuron[w][i] = 1
      else:
        dict_winner_neuron[w][i] += 1
  return dict_winner_neuron

def m_xy(winner_neurons_dict):
  return_dict = {}
  for i in winner_neurons_dict.keys():
    x = 0 
    for j in winner_neurons_dict[i].keys():
      x += winner_neurons_dict[i][j]
    return_dict[i] = x
  return return_dict
    

def normalized_metric(winner_neurons_dict, num_elements_per_class):
  m_xy1 = m_xy(winner_neurons_dict)
  
  return_dict, p_xy, q_xy, pq_xy = {}, {}, {}, {}
  
  for a in winner_neurons_dict.keys():
    if a not in p_xy:
      p_xy[a] = {}
    if a not in q_xy:
      q_xy[a] = {} 
    if a not in pq_xy:
      pq_xy[a] = {}

    for b in winner_neurons_dict[a].keys():
      if b not in p_xy:
        p_xy[a][b] = float(winner_neurons_dict[a][b]) / float(num_elements_per_class[b])
      if b not in q_xy:
        q_xy[a][b] = float(winner_neurons_dict[a][b]) / float(m_xy1[a])
      if b not in pq_xy:
        pq_xy[a][b] = p_xy[a][b] * q_xy[a][b]
      if b not in return_dict:
        return_dict[b] = pq_xy[a][b]
      return_dict[b] += pq_xy[a][b]

  ret_val = 0
  
  for c in return_dict:
    ret_val += return_dict[c]
  
  return ret_val/len(return_dict.keys())


def majority_class_neuron(winner_neuron_dict):
  majority_dict = {}
  for location in winner_neuron_dict:
    majority_dict[location] = max(winner_neuron_dict[location], key=winner_neuron_dict[location].get)
  return majority_dict

def classification_metric(winner_neuron_dict, som, X_test):
  y_pred = []
  majority_class_nron = majority_class_neuron(winner_neuron_dict)
  for x_i in X_test:
    location = som.winner(x_i)
    y_pred.append(str(majority_class_nron[location]))
  return y_pred

# Data Declaration & Initialization

Set data.

data_x, data_y = fetch_openml('mnist_784', version = 1, return_X_y = True)
data_x = data_x / 255

num_hw_digits = 10

sigma6x6 = 'sigma6x6'
sigma12x12 = 'sigma12x12'
sigma24x24 = 'sigma24x24'

lr6x6 = 'lr6x6'
lr12x12 = 'lr12x12'
lr24x24 = 'lr24x24'

PARAM = { sigma6x6: 3.00, sigma12x12: 3.00, sigma24x24: 3.00, lr6x6: 0.100, lr12x12: 0.100, lr24x24: 0.100 }

Initialize the data.

data_x_600 = []
data_y_600 = []
data_x_6000 = []
data_y_6000 = []

for i in range(num_hw_digits):
  data_x_600.append(data_x[data_y == str(i)][:60])
  data_y_600 += [str(i)]*60
  data_x_6000.append(data_x[data_y == str(i)][:600])
  data_y_6000 += [str(i)]*600

Randomize the elements.

data_x_6000 = data_x[:60000]
data_y_6000 = data_y[:60000]
data_x_600 = data_x[60000:70000]
data_y_600 = data_y[60000:70000]

# Data Implementation

# Benchmarking

## Implementation

som6x6_BM = MiniSom(6, 6, 784, sigma = PARAM[sigma6x6], learning_rate = PARAM[lr6x6])
som6x6_BM.train_random(data_x_6000.values.tolist(), 4000, verbose = True)

weight6x6_BM = som6x6_BM.get_weights().reshape(6*6, 28, 28).copy()
ig, axes = plt.subplots(6, 6, figsize = (6, 6), sharex = True, sharey = True)

for i in range(6*6):
  subplot_row = i // 6
  subplot_col = i % 6
  ax = axes[subplot_row, subplot_col]
  
  plottable_image = weight6x6_BM[i]
  ax.imshow(plottable_image, cmap='gray_r')

  ax.set_xbound([0, 28])

plt.tight_layout()
plt.show()

som12x12_BM = MiniSom(12, 12, 784, sigma = PARAM[sigma12x12], learning_rate = PARAM[lr12x12])
som12x12_BM.train_random(data_x_6000.values.tolist(), 8000, verbose = True)

weights12x12_BM = som12x12_BM.get_weights().reshape(12*12, 28, 28).copy()
ig, axes = plt.subplots(12, 12, figsize = (12, 12), sharex = True, sharey = True)

for i in range(12*12):
  subplot_row = i // 12
  subplot_col = i % 12

  ax = axes[subplot_row, subplot_col]
  
  plottable_image = weights12x12_BM[i]
  
  ax.imshow(plottable_image, cmap='gray_r')
  ax.set_xbound([0, 28])

plt.tight_layout()
plt.show()

som24x24_BM = MiniSom(24, 24, 784, sigma = PARAM[sigma24x24], learning_rate = PARAM[lr24x24])
som24x24_BM.train_random(data_x_6000.values.tolist(), 12000, verbose = True)

weights24x24_BM = som24x24_BM.get_weights().reshape(24*24, 28, 28).copy()
ig, axes = plt.subplots(24, 24, figsize = (24, 24), sharex = True, sharey = True)

for i in range(24*24):
  subplot_row = i // 24
  subplot_col = i % 24
  ax = axes[subplot_row, subplot_col]

  plottable_image = weights24x24_BM[i]
  ax.imshow(plottable_image, cmap='gray_r')

  ax.set_xbound([0, 28])

plt.tight_layout()
plt.show()

dict_WN_6x6_BM_train = dict_for_entropy(som6x6_BM, np.asarray(data_x_6000), np.asarray(data_y_6000))
dict_WN_6x6_BM_test = dict_for_entropy(som6x6_BM, np.asarray(data_x_600), np.asarray(data_y_600))

dict_WN_12x12_BM_train = dict_for_entropy(som12x12_BM, np.asarray(data_x_6000), np.asarray(data_y_6000))
dict_WN_12x12_BM_test = dict_for_entropy(som12x12_BM, np.asarray(data_x_600), np.asarray(data_y_600))

dict_WN_24x24_BM_train = dict_for_entropy(som24x24_BM, np.asarray(data_x_6000), np.asarray(data_y_6000))
dict_WN_24x24_BM_test = dict_for_entropy(som24x24_BM, np.asarray(data_x_600), np.asarray(data_y_600))

train_elements_BM = {}
test_elements_BM = {}

for j in range(10):
  x = 0
  for i in (data_y_6000 == str(j)):
    if i:
      x += 1
  train_elements_BM[j] = x
  
  x = 0
  for i in (data_y_600 == str(j)):
    if i:
      x += 1
  test_elements_BM[j] = x

## Normalization Metric

print(normalized_metric(dict_WN_6x6_BM_train, train_elements_BM),
      normalized_metric(dict_WN_12x12_BM_train, train_elements_BM),
      normalized_metric(dict_WN_24x24_BM_train, train_elements_BM))

print(normalized_metric(dict_WN_6x6_BM_test, test_elements_BM),
      normalized_metric(dict_WN_12x12_BM_test, test_elements_BM),
      normalized_metric(dict_WN_24x24_BM_test, test_elements_BM))

## Classification Metric

y_pred_som6x6_BM = classification_metric(dict_WN_6x6_BM_test, som6x6_BM, np.asarray(data_x_600))
y_true_som6x6_BM = classification_metric(dict_WN_6x6_BM_train, som6x6_BM, np.asarray(data_x_6000))

print('-----------------------------------\n\n',
      cm(data_y_600, y_pred_som6x6_BM), '\n\n',
      cr(data_y_600, y_pred_som6x6_BM), '\n',
      accuracy(data_y_600, y_pred_som6x6_BM))

print('-----------------------------------\n\n',
      cm(data_y_6000, y_true_som6x6_BM), '\n\n',
      cr(data_y_6000, y_true_som6x6_BM), '\n',
      accuracy(data_y_6000, y_true_som6x6_BM))

y_pred_som12x12_BM = classification_metric(dict_WN_12x12_BM_test, som12x12_BM, np.asarray(data_x_600))
y_true_som12x12_BM = classification_metric(dict_WN_12x12_BM_train, som12x12_BM, np.asarray(data_x_6000))

print('-----------------------------------\n\n',
      cm(data_y_600, y_pred_som12x12_BM), '\n\n',
      cr(data_y_600, y_pred_som12x12_BM), '\n',
      accuracy(data_y_600, y_pred_som12x12_BM))

print('-----------------------------------\n\n',
      cm(data_y_6000, y_true_som12x12_BM), '\n\n',
      cr(data_y_6000, y_true_som12x12_BM), '\n',
      accuracy(data_y_6000, y_true_som12x12_BM))

y_pred_som24x24_BM = classification_metric(dict_WN_24x24_BM_test, som24x24_BM, np.asarray(data_x_600))
y_true_som24x24_BM = classification_metric(dict_WN_24x24_BM_train, som24x24_BM, np.asarray(data_x_6000))

print('-----------------------------------\n\n',
      cm(data_y_600, y_pred_som24x24_BM), '\n\n',
      cr(data_y_600, y_pred_som24x24_BM), '\n',
      accuracy(data_y_600, y_pred_som24x24_BM))

print('-----------------------------------\n\n',
      cm(data_y_6000, y_true_som24x24_BM), '\n\n',
      cr(data_y_6000, y_true_som24x24_BM), '\n',
      accuracy(data_y_6000, y_true_som24x24_BM))

# DevSOM Replication

som6x6_DSR = MiniSom(6, 6, 784, sigma = PARAM[sigma6x6], learning_rate = PARAM[lr6x6])
som6x6_DSR.train_random(data_x_6000.values.tolist(), 4000, verbose = True)

weight6x6_DSR = som6x6_DSR.get_weights().reshape(6*6, 28, 28).copy()
ig, axes = plt.subplots(6, 6, figsize = (6, 6), sharex = True, sharey = True)

for i in range(6*6):
  subplot_row = i // 6
  subplot_col = i % 6
  ax = axes[subplot_row, subplot_col]
  
  plottable_image = weight6x6_DSR[i]
  ax.imshow(plottable_image, cmap='gray_r')

  ax.set_xbound([0, 28])

plt.tight_layout()
plt.show()

x = som6x6_DSR._weights.copy()
print(x.shape)
x = np.repeat(x, 2, axis=1)
x = np.repeat(x, 2, axis=0)
print(x.shape)
random_matrix = np.random.rand(12,12,784)*0.2 - 0.05

x = x + random_matrix

som12x12_DSR = MiniSom(12, 12, 784, sigma = PARAM[sigma12x12], learning_rate = PARAM[lr12x12])
som12x12_DSR._weights = x.copy()
som12x12_DSR.train_random(data_x_6000.values.tolist(), 8000, verbose = True)

weights12x12_DSR = som12x12_DSR.get_weights().reshape(12*12, 28, 28).copy()
ig, axes = plt.subplots(12, 12, figsize = (12, 12), sharex = True, sharey = True)

for i in range(12*12):
  subplot_row = i // 12
  subplot_col = i % 12

  ax = axes[subplot_row, subplot_col]
  
  plottable_image = weights12x12_DSR[i]
  
  ax.imshow(plottable_image, cmap='gray_r')
  ax.set_xbound([0, 28])

plt.tight_layout()
plt.show()

x1 = som12x12_DSR._weights.copy()
print(x1.shape)
x1 = np.repeat(x1, 2, axis=1)
x1 = np.repeat(x1, 2, axis=0)
print(x1.shape)
random_matrix = np.random.rand(24,24,784)*0.2 - 0.05

x1 = x1 + random_matrix

som24x24_DSR = MiniSom(24, 24, 784, sigma = PARAM[sigma24x24], learning_rate = PARAM[lr24x24])
som24x24_DSR._weights = x1.copy()
som24x24_DSR.train_random(data_x_6000.values.tolist(), 12000, verbose = True)

weights24x24_DSR = som24x24_DSR.get_weights().reshape(24*24, 28, 28).copy()
ig, axes = plt.subplots(24, 24, figsize = (24, 24), sharex = True, sharey = True)
for i in range(24*24):
  subplot_row = i // 24
  subplot_col = i % 24
  ax = axes[subplot_row, subplot_col]

  plottable_image = weights24x24_DSR[i]
  ax.imshow(plottable_image, cmap='gray_r')

  ax.set_xbound([0, 28])

plt.tight_layout()
plt.show()

## DevSOM Performance Metrics



dict_WN_6x6_DSR_train = dict_for_entropy(som6x6_DSR, np.asarray(data_x_6000), np.asarray(data_y_6000))
dict_WN_6x6_DSR_test = dict_for_entropy(som6x6_DSR, np.asarray(data_x_600), np.asarray(data_y_600))

dict_WN_12x12_DSR_train = dict_for_entropy(som12x12_DSR, np.asarray(data_x_6000), np.asarray(data_y_6000))
dict_WN_12x12_DSR_test = dict_for_entropy(som12x12_DSR, np.asarray(data_x_600), np.asarray(data_y_600))

dict_WN_24x24_DSR_train = dict_for_entropy(som24x24_DSR, np.asarray(data_x_6000), np.asarray(data_y_6000))
dict_WN_24x24_DSR_test = dict_for_entropy(som24x24_DSR, np.asarray(data_x_600), np.asarray(data_y_600))

train_elements_DSR = {}
test_elements_DSR = {}

for j in range(10):
  x = 0
  for i in (data_y_6000 == str(j)):
    if i:
      x += 1
  train_elements_DSR[j] = x
  
  x = 0
  for i in (data_y_600 == str(j)):
    if i:
      x += 1
  test_elements_DSR[j] = x

## Normalized Metric

print(normalized_metric(dict_WN_6x6_DSR_train, train_elements_DSR),
      normalized_metric(dict_WN_12x12_DSR_train, train_elements_DSR),
      normalized_metric(dict_WN_24x24_DSR_train, train_elements_DSR))

print(normalized_metric(dict_WN_6x6_DSR_test, test_elements_DSR),
      normalized_metric(dict_WN_12x12_DSR_test, test_elements_DSR),
      normalized_metric(dict_WN_24x24_DSR_test, test_elements_DSR))

## Classification Metric

y_pred_som6x6_DSR = classification_metric(dict_WN_6x6_DSR_test, som6x6_DSR, np.asarray(data_x_600))
y_true_som6x6_DSR = classification_metric(dict_WN_6x6_DSR_train, som6x6_DSR, np.asarray(data_x_6000))

print('-----------------------------------\n\n',
      cm(data_y_600, y_pred_som6x6_DSR), '\n\n',
      cr(data_y_600, y_pred_som6x6_DSR), '\n',
      accuracy(data_y_600, y_pred_som6x6_DSR))

print('-----------------------------------\n\n',
      cm(data_y_6000, y_true_som6x6_DSR), '\n\n',
      cr(data_y_6000, y_true_som6x6_DSR), '\n',
      accuracy(data_y_6000, y_true_som6x6_DSR))

y_pred_som12x12_DSR = classification_metric(dict_WN_12x12_DSR_test, som12x12_DSR, np.asarray(data_x_600))
y_true_som12x12_DSR = classification_metric(dict_WN_12x12_DSR_train, som12x12_DSR, np.asarray(data_x_6000))

print('-----------------------------------\n\n',
      cm(data_y_600, y_pred_som12x12_DSR), '\n\n',
      cr(data_y_600, y_pred_som12x12_DSR), '\n',
      accuracy(data_y_600, y_pred_som12x12_DSR))

print('-----------------------------------\n\n',
      cm(data_y_6000, y_true_som12x12_DSR), '\n\n',
      cr(data_y_6000, y_true_som12x12_DSR), '\n',
      accuracy(data_y_6000, y_true_som12x12_DSR))

y_pred_som24x24_DSR = classification_metric(dict_WN_24x24_DSR_test, som24x24_DSR, np.asarray(data_x_600))
y_true_som24x24_DSR = classification_metric(dict_WN_24x24_DSR_train, som24x24_DSR, np.asarray(data_x_6000))

print('-----------------------------------\n\n',
      cm(data_y_600, y_pred_som24x24_DSR), '\n\n',
      cr(data_y_600, y_pred_som24x24_DSR), '\n',
      accuracy(data_y_600, y_pred_som24x24_DSR))

print('-----------------------------------\n\n',
      cm(data_y_6000, y_true_som24x24_DSR), '\n\n',
      cr(data_y_6000, y_true_som24x24_DSR), '\n',
      accuracy(data_y_6000, y_true_som24x24_DSR))

# Downsampling

data_x_6000.shape

data_x_6000_DS = block_reduce(np.asarray(data_x_6000), block_size=(2, 2), func=np.mean)
data_x_6000_DS = np.repeat(data_x_6000_DS, 2, axis=1)
data_x_6000_DS = np.repeat(data_x_6000_DS, 2, axis=0)

print(data_x_6000_DS.shape)


som6x6_ds = MiniSom(6, 6, 784, sigma=PARAM[sigma6x6], learning_rate=PARAM[lr6x6])
som6x6_ds.train_random(data_x_6000_DS, 4000, verbose=True)

weight6x6_ds = som6x6_ds.get_weights().reshape(6*6, 28, 28).copy()
ig6x6_ds, axes6x6_ds = plt.subplots(6, 6, figsize = (6, 6), sharex = True, sharey = True)

for i in range(6*6):
  subplot_row = i // 6
  subplot_col = i % 6
  ax = axes6x6_ds[subplot_row, subplot_col]

  plottable_image = weight6x6_ds[i]
  ax.imshow(plottable_image, cmap='gray_r')

  ax.set_xbound([0, 28])

plt.tight_layout()
plt.show()

som12x12_ds = MiniSom(12, 12, 784, sigma=PARAM[sigma12x12], learning_rate=PARAM[lr12x12])
som12x12_ds.train_random(data_x_6000_DS, 8000, verbose=True)

weight12x12_ds = som12x12_ds.get_weights().reshape(12*12, 28, 28).copy()
ig12x12_ds, axes12x12_ds = plt.subplots(12, 12, figsize = (12, 12), sharex = True, sharey = True)

for i in range(12*12):
  subplot_row = i // 12
  subplot_col = i % 12
  ax = axes12x12_ds[subplot_row, subplot_col]

  plottable_image = weight12x12_ds[i]
  ax.imshow(plottable_image, cmap='gray_r')

  ax.set_xbound([0, 28])

plt.tight_layout()
plt.show()

som24x24_ds = MiniSom(24, 24, 784, sigma=PARAM[sigma24x24], learning_rate=PARAM[lr24x24])
som24x24_ds.train_random(data_x_6000_DS, 12000, verbose=True)

weight24x24_ds = som24x24_ds.get_weights().reshape(24*24, 28, 28).copy()
ig24x24_ds, axes24x24_ds = plt.subplots(24, 24, figsize = (24, 24), sharex = True, sharey = True)

for i in range(24*24):
  subplot_row = i // 24
  subplot_col = i % 24
  ax = axes24x24_ds[subplot_row, subplot_col]

  plottable_image = weight24x24_ds[i]
  ax.imshow(plottable_image, cmap='gray_r')

  ax.set_xbound([0, 28])

plt.tight_layout()
plt.show()

## Downsampling Perfomance Metrics

dict_WN_6x6_ds_train = dict_for_entropy(som6x6_ds, np.asarray(data_x_6000), np.asarray(data_y_6000))
dict_WN_6x6_ds_test = dict_for_entropy(som6x6_ds, np.asarray(data_x_600), np.asarray(data_y_600))

dict_WN_12x12_ds_train = dict_for_entropy(som12x12_ds, np.asarray(data_x_6000), np.asarray(data_y_6000))
dict_WN_12x12_ds_test = dict_for_entropy(som12x12_ds, np.asarray(data_x_600), np.asarray(data_y_600))

dict_WN_24x24_ds_train = dict_for_entropy(som24x24_ds, np.asarray(data_x_6000), np.asarray(data_y_6000))
dict_WN_24x24_ds_test = dict_for_entropy(som24x24_ds, np.asarray(data_x_600), np.asarray(data_y_600))

train_elements_ds = {}
test_elements_ds = {}

for j in range(10):
  x = 0
  for i in (data_y_6000 == str(j)):
    if i:
      x += 1
  train_elements_ds[j] = x
  
  x = 0
  for i in (data_y_600 == str(j)):
    if i:
      x += 1
  test_elements_ds[j] = x

## Normalized Metric

print(normalized_metric(dict_WN_6x6_ds_train, train_elements_ds),
      normalized_metric(dict_WN_12x12_ds_train, train_elements_ds),
      normalized_metric(dict_WN_24x24_ds_train, train_elements_ds))

print(normalized_metric(dict_WN_6x6_ds_test, test_elements_ds),
      normalized_metric(dict_WN_12x12_ds_test, test_elements_ds),
      normalized_metric(dict_WN_24x24_ds_test, test_elements_ds))

## Classification Metric

y_pred_som6x6_ds = classification_metric(dict_WN_6x6_ds_test, som6x6_ds, np.asarray(data_x_600))
y_true_som6x6_ds = classification_metric(dict_WN_6x6_ds_train, som6x6_ds, np.asarray(data_x_6000))

print('-----------------------------------\n\n',
      cm(data_y_600, y_pred_som6x6_ds), '\n\n',
      cr(data_y_600, y_pred_som6x6_ds), '\n',
      accuracy(data_y_600, y_pred_som6x6_ds))

print('-----------------------------------\n\n',
      cm(data_y_6000, y_true_som6x6_ds), '\n\n',
      cr(data_y_6000, y_true_som6x6_ds), '\n',
      accuracy(data_y_6000, y_true_som6x6_ds))

y_pred_som12x12_ds = classification_metric(dict_WN_12x12_ds_test, som12x12_ds, np.asarray(data_x_600))
y_true_som12x12_ds = classification_metric(dict_WN_12x12_ds_train, som12x12_ds, np.asarray(data_x_6000))

print('-----------------------------------\n\n',
      cm(data_y_600, y_pred_som12x12_ds), '\n\n',
      cr(data_y_600, y_pred_som12x12_ds), '\n',
      accuracy(data_y_600, y_pred_som12x12_ds))

print('-----------------------------------\n\n',
      cm(data_y_6000, y_true_som12x12_ds), '\n\n',
      cr(data_y_6000, y_true_som12x12_ds), '\n',
      accuracy(data_y_6000, y_true_som12x12_ds))

y_pred_som24x24_ds = classification_metric(dict_WN_24x24_ds_test, som24x24_ds, np.asarray(data_x_600))
y_true_som24x24_ds = classification_metric(dict_WN_24x24_ds_train, som24x24_ds, np.asarray(data_x_6000))

print('-----------------------------------\n\n',
      cm(data_y_600, y_pred_som24x24_ds), '\n\n',
      cr(data_y_600, y_pred_som24x24_ds), '\n',
      accuracy(data_y_600, y_pred_som24x24_ds))

print('-----------------------------------\n\n',
      cm(data_y_6000, y_true_som24x24_ds), '\n\n',
      cr(data_y_6000, y_true_som24x24_ds), '\n',
      accuracy(data_y_6000, y_true_som24x24_ds))
